{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import pandas as pd \n",
    "# from sqlalchemy.types import Integer, Text, String, DateTime\n",
    "import sqlalchemy as s\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "with open(\"config.json\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "database_connection_string = 'postgres+psycopg2://{}:{}@{}:{}/{}'.format(config['user'], config['password'], config['host'], config['port'], config['database'])\n",
    "\n",
    "dbschema='augur_data'\n",
    "engine = s.create_engine(\n",
    "    database_connection_string,\n",
    "    connect_args={'options': '-csearch_path={}'.format(dbschema)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   repo_id  repo_name                              repo_path\n",
      "0    26235  concourse               github.com/pcfdev-forks/\n",
      "1    28051  concourse                  github.com/concourse/\n",
      "2    27913    clarity                     github.com/vmware/\n",
      "3    26983  postfacto                    github.com/pivotal/\n",
      "4    27169       gpdb                github.com/pivotal-gss/\n",
      "5    25857       gpdb               github.com/greenplum-db/\n",
      "6    26600       gpdb  github.com/Pivotal-Field-Engineering/\n",
      "7    27043      kpack                    github.com/pivotal/\n"
     ]
    }
   ],
   "source": [
    "repo_list = pd.DataFrame()\n",
    "repo_list_query = f\"\"\"\n",
    "SELECT repo_id, repo_name, repo_path from repo\n",
    "WHERE repo_name = 'concourse' OR repo_name = 'postfacto' or repo_name = 'clarity' or repo_name = 'gpdb' or\n",
    "      repo_name = 'kpack';\n",
    "    \"\"\"\n",
    "repo_list = pd.read_sql_query(repo_list_query, con=engine)\n",
    "print(repo_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_set = {26983}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   repo_id  repo_name  rg_name  issues_count\n",
      "0    26983  postfacto  pivotal           110\n"
     ]
    }
   ],
   "source": [
    "pr_all = pd.DataFrame()\n",
    "# \n",
    "for repo_id in repo_set: \n",
    "\n",
    "    pr_query = f\"\"\"\n",
    "    SELECT\n",
    "        repo.repo_id,\n",
    "        repo.repo_name,\n",
    "        repo_groups.rg_name,\n",
    "        E.issues_count\n",
    "    FROM\n",
    "        repo\n",
    "        LEFT OUTER JOIN (\n",
    "        SELECT\n",
    "            issues.issue_id,\n",
    "            issues.repo_id \n",
    "        FROM\n",
    "            issues\n",
    "            LEFT OUTER JOIN issue_message_ref K ON issues.issue_id = K.issue_id \n",
    "        WHERE\n",
    "            pull_request IS NULL -- GitHub provides pull requests in their issues API, as well as their pull requests API. We do not exclude this data from collection because it would make the provenance of the data we collect less transparent. We apply filters in queries and API endpoints, but not collection.\n",
    "            \n",
    "        GROUP BY\n",
    "            issues.issue_id,\n",
    "            issues.repo_id \n",
    "        ORDER BY\n",
    "            issues.repo_id \n",
    "        ) D ON repo.repo_id = D.repo_id,\n",
    "        repo_groups,\n",
    "        ( -- subquery table to provide issues count in context \n",
    "        SELECT\n",
    "            repo.repo_id,\n",
    "            COUNT ( issue_id ) AS issues_count \n",
    "        FROM\n",
    "            repo\n",
    "            LEFT OUTER JOIN (\n",
    "            SELECT\n",
    "                repo.repo_id,\n",
    "                issues.issue_id --the \"double left outer join here seems puzzling. TO preserve \"one row per repo\" and exclude pull requests, we FIRST need to get a list of issues that are not pull requests, then count those. WIthout the \"double left outer join\", we would exclude repos that use pull requests, but not issues on GitHub\n",
    "                \n",
    "            FROM\n",
    "                repo\n",
    "                LEFT OUTER JOIN issues ON issues.repo_id = repo.repo_id \n",
    "            WHERE\n",
    "                issues.pull_request IS NULL -- here again, excluding pull_requests at data analysis, but preserving GitHub API Provenance\n",
    "                \n",
    "            ) K ON repo.repo_id = K.repo_id \n",
    "        GROUP BY\n",
    "            repo.repo_id \n",
    "        ) E -- this subquery table is what gives us the issue count per repo as context for deciding if repos with very small issue counts are excluded from some analyses.\n",
    "        \n",
    "    WHERE\n",
    "        repo.repo_group_id = repo_groups.repo_group_id \n",
    "        AND repo.repo_id = E.repo_id \n",
    "        AND repo.repo_id = {repo_id}\n",
    "    GROUP BY\n",
    "        repo.repo_id,\n",
    "        repo.repo_name,\n",
    "        repo_groups.rg_name,\n",
    "        repo_groups.repo_group_id,\n",
    "        E.issues_count \n",
    "    ORDER BY\n",
    "        rg_name,\n",
    "        repo_name;\n",
    "\n",
    "        \"\"\"\n",
    "    pr_a = pd.read_sql(pr_query, con=engine)\n",
    "    if not pr_all.empty: \n",
    "        pr_all = pd.concat([pr_all, pr_a]) \n",
    "    else: \n",
    "        # first repo\n",
    "        pr_all = pr_a\n",
    "        \n",
    "print(pr_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'2020-03-12'\n",
      "'2020-02-29'\n",
      "'2019-03-01'\n"
     ]
    }
   ],
   "source": [
    "#from datetime import date\n",
    "import datetime \n",
    "\n",
    "current = datetime.date.today()\n",
    "today = \"'\" + str(current) + \"'\"\n",
    "print(today)\n",
    "\n",
    "first_current = current.replace(day=1)\n",
    "last_month = first_current - datetime.timedelta(days=1)\n",
    "end_date = \"'\" + str(last_month) + \"'\"\n",
    "print(end_date)\n",
    "\n",
    "print\n",
    "\n",
    "start = last_month - datetime.timedelta(days=365)\n",
    "year_ago = \"'\" + str(start) + \"'\"\n",
    "print(year_ago)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    issue_id                                        issue_title  repo_id  \\\n",
      "0     387660  Heroku deploy doesn't allow users to log in to...    26983   \n",
      "1     387653  Retro list page errors when auth token out of ...    26983   \n",
      "2     387650                Investigate \"forced logout\" feature    26983   \n",
      "3     387649  Felicity wants creating an action to leave her...    26983   \n",
      "4     387648                                 Oh no! It's broken    26983   \n",
      "5     387647            Use of sed in deploy.sh is mac-specific    26983   \n",
      "6     387646  Deploy.sh incorrectly filling/missing placehol...    26983   \n",
      "7     387645  PG::UndefinedColumn: ERROR:  column \"auth_toke...    26983   \n",
      "8     387643                  Heroku deploy script doesn't work    26983   \n",
      "9     387642                       Ruby version 2.6.1 not found    26983   \n",
      "10    387639  Ability to vote on items while another one is ...    26983   \n",
      "11    387638  PWS: frontend and backend cannot connect becau...    26983   \n",
      "12    387636  Investigate how we might reduce dependencies w...    26983   \n",
      "13    387633       \"Could not find 'bundler'\" on PCF deployment    26983   \n",
      "14    387630  Core team wants to avoid cutting a release if ...    26983   \n",
      "15    387629                Proposal: Replace p-flux with redux    26983   \n",
      "16    387627  bad URI(is not URI?): \"redis://:...\" (URI::Inv...    26983   \n",
      "17    387625                          Replace p-flux with redux    26983   \n",
      "18    387623  Investigate if it is possible to run 2 dynos f...    26983   \n",
      "19    387618               Flaky end-to-end tests/timing issues    26983   \n",
      "20    387613                         Unable to deploy postfacto    26983   \n",
      "21    387611             Sizing issue with retro item input box    26983   \n",
      "22    387598                   Local development docker compose    26983   \n",
      "23    387597  LoadError: cannot load such file -- coffee_script    26983   \n",
      "24    387594            add a simple way to generate an api key    26983   \n",
      "25    387589                             Containerize postfacto    26983   \n",
      "26    387587  Heroku deploy: The page you were looking for d...    26983   \n",
      "27    387581             Formatting issue with multi line items    26983   \n",
      "28    387579       Split Travis build into multiple stages/jobs    26983   \n",
      "29    387578  Creating a new retro from the Admin UI does no...    26983   \n",
      "30    387573  ActionView::MissingTemplate error after migrat...    26983   \n",
      "31    387570       Scroll does not work in Chrome on Windows 10    26983   \n",
      "32    387567  Can't archive a retrospective when there's no ...    26983   \n",
      "\n",
      "   pull_request          created_at          updated_at  \n",
      "0          None 2019-03-01 13:18:32 2019-03-20 13:34:28  \n",
      "1          None 2019-03-19 14:39:37 2019-03-19 15:50:38  \n",
      "2          None 2019-03-28 21:39:26 2019-11-19 13:58:37  \n",
      "3          None 2019-04-04 16:53:08 2019-04-04 16:53:08  \n",
      "4          None 2019-04-12 15:50:39 2019-04-12 20:28:49  \n",
      "5          None 2019-04-20 17:29:32 2019-07-18 12:57:24  \n",
      "6          None 2019-04-20 17:29:39 2019-07-11 15:36:13  \n",
      "7          None 2019-04-29 09:29:39 2019-07-11 15:39:21  \n",
      "8          None 2019-05-06 17:07:16 2019-07-18 12:53:50  \n",
      "9          None 2019-05-09 22:24:10 2019-07-11 15:20:08  \n",
      "10         None 2019-05-22 19:05:34 2019-07-11 16:01:14  \n",
      "11         None 2019-06-02 09:06:57 2019-10-25 15:03:53  \n",
      "12         None 2019-06-28 10:39:23 2019-06-28 10:39:39  \n",
      "13         None 2019-07-12 10:34:58 2019-07-16 15:27:44  \n",
      "14         None 2019-07-12 14:32:52 2019-08-13 16:27:24  \n",
      "15         None 2019-07-13 13:47:50 2019-07-15 18:27:42  \n",
      "16         None 2019-07-15 16:45:02 2019-07-17 13:03:09  \n",
      "17         None 2019-07-16 09:19:21 2019-07-29 16:00:55  \n",
      "18         None 2019-07-16 11:08:14 2019-07-16 13:39:42  \n",
      "19         None 2019-07-19 16:55:58 2019-10-25 15:46:19  \n",
      "20         None 2019-07-29 05:04:26 2019-07-29 18:48:49  \n",
      "21         None 2019-07-29 12:34:11 2019-08-06 09:44:49  \n",
      "22         None 2019-08-14 16:18:30 2019-08-15 10:34:17  \n",
      "23         None 2019-08-16 08:43:01 2019-08-16 08:43:13  \n",
      "24         None 2019-09-04 17:15:05 2019-10-25 15:28:01  \n",
      "25         None 2019-10-22 08:18:52 2019-11-06 13:17:37  \n",
      "26         None 2019-10-24 14:56:28 2019-10-25 11:30:53  \n",
      "27         None 2019-10-27 21:19:19 2019-10-27 21:19:47  \n",
      "28         None 2019-11-04 22:44:34 2019-11-06 13:15:54  \n",
      "29         None 2019-11-14 20:22:29 2019-11-19 05:11:27  \n",
      "30         None 2019-11-28 15:23:31 2019-12-12 16:37:21  \n",
      "31         None 2019-12-17 22:32:40 2020-01-03 15:21:33  \n",
      "32         None 2020-01-15 13:57:15 2020-02-20 19:36:09  \n"
     ]
    }
   ],
   "source": [
    "pr_all = pd.DataFrame()\n",
    "# \n",
    "for repo_id in repo_set: \n",
    "\n",
    "    pr_query = f\"\"\"\n",
    "    SELECT issue_id, issue_title, repo_id, pull_request, created_at, updated_at FROM issues\n",
    "    WHERE\n",
    "        repo_id = {repo_id}\n",
    "        AND pull_request is NULL\n",
    "        AND created_at >= {year_ago}\n",
    "        AND created_at <= {end_date}\n",
    "    ORDER BY\n",
    "        created_at;\n",
    "    \"\"\"\n",
    "    pr_a = pd.read_sql(pr_query, con=engine)\n",
    "    if not pr_all.empty: \n",
    "        pr_all = pd.concat([pr_all, pr_a]) \n",
    "    else: \n",
    "        # first repo\n",
    "        pr_all = pr_a\n",
    "        \n",
    "print(pr_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    issue_id          created_at            action          created_at\n",
      "0     387567 2020-01-15 13:57:15         mentioned 2020-01-30 14:15:21\n",
      "1     387567 2020-01-15 13:57:15        subscribed 2020-01-30 14:15:21\n",
      "2     387567 2020-01-15 13:57:15         mentioned 2020-01-30 15:15:56\n",
      "3     387567 2020-01-15 13:57:15        subscribed 2020-01-30 15:15:56\n",
      "4     387567 2020-01-15 13:57:15  added_to_project 2020-02-03 18:43:34\n",
      "5     387567 2020-01-15 13:57:15           labeled 2020-02-03 18:43:38\n",
      "6     387567 2020-01-15 13:57:15         mentioned 2020-02-03 18:43:41\n",
      "7     387567 2020-01-15 13:57:15        subscribed 2020-02-03 18:43:41\n",
      "8     387567 2020-01-15 13:57:15         mentioned 2020-02-03 19:06:21\n",
      "9     387567 2020-01-15 13:57:15        subscribed 2020-02-03 19:06:21\n",
      "10    387567 2020-01-15 13:57:15         mentioned 2020-02-03 19:06:21\n",
      "11    387567 2020-01-15 13:57:15        subscribed 2020-02-03 19:06:21\n",
      "12    387567 2020-01-15 13:57:15         mentioned 2020-02-08 10:46:46\n",
      "13    387567 2020-01-15 13:57:15        subscribed 2020-02-08 10:46:46\n",
      "14    387567 2020-01-15 13:57:15         mentioned 2020-02-11 15:42:38\n",
      "15    387567 2020-01-15 13:57:15        subscribed 2020-02-11 15:42:38\n",
      "16    387567 2020-01-15 13:57:15         mentioned 2020-02-12 09:08:09\n",
      "17    387567 2020-01-15 13:57:15        subscribed 2020-02-12 09:08:09\n",
      "18    387567 2020-01-15 13:57:15         mentioned 2020-02-20 19:36:09\n",
      "19    387567 2020-01-15 13:57:15        subscribed 2020-02-20 19:36:09\n",
      "20    387567 2020-01-15 13:57:15         mentioned 2020-02-20 19:36:09\n",
      "21    387567 2020-01-15 13:57:15        subscribed 2020-02-20 19:36:09\n"
     ]
    }
   ],
   "source": [
    "pr_all = pd.DataFrame()\n",
    "# \n",
    "for repo_id in repo_set: \n",
    "\n",
    "    pr_query = f\"\"\"\n",
    "    SELECT issues.issue_id, issues.created_at, issue_events.action, issue_events.created_at\n",
    "    FROM issues, issue_events\n",
    "    WHERE\n",
    "        issues.issue_id = issue_events.issue_id\n",
    "        AND issues.repo_id = {repo_id}\n",
    "        AND issues.pull_request is NULL\n",
    "        AND issues.created_at >= {year_ago}\n",
    "        AND issues.created_at <= {end_date}\n",
    "    ORDER BY\n",
    "        issues.created_at;\n",
    "    \"\"\"\n",
    "    pr_a = pd.read_sql(pr_query, con=engine)\n",
    "    if not pr_all.empty: \n",
    "        pr_all = pd.concat([pr_all, pr_a]) \n",
    "    else: \n",
    "        # first repo\n",
    "        pr_all = pr_a\n",
    "        \n",
    "print(pr_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
